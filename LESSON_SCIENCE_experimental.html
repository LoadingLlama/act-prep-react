<!--
LESSON TEMPLATE v3.0
Subject: Science
Topic: Experimental Setup and Design
Lesson Key: experimental-setup
-->

<!-- ========================================
     SECTION 1: OPENING PARAGRAPH
     ======================================== -->

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
Experimental setup questions test your ability to <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">understand experimental design principles</strong>, <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">identify controlled variables and constants</strong>, <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">recognize the purpose of control groups</strong>, <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">evaluate whether an experiment tests its stated hypothesis</strong>, and <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">determine what changes would improve experimental validity</strong>. These questions appear across all ACT Science passage types, asking you to analyze how experiments are structured, why specific procedures are followed, and what modifications would better test hypotheses or eliminate confounding variables. This lesson teaches you the fundamental components of scientific experiments and how to identify them in passages, explains the logic behind experimental controls and variable manipulation, provides strategies for recognizing flawed designs and suggesting improvements, and shows you how to match experimental procedures to research questions and hypotheses.
</p>

<!-- ========================================
     SECTION 2: CONTENT (EXACTLY 4 H3 SECTIONS)
     ======================================== -->

<h3 style="margin-top: 5rem; margin-bottom: 0.75rem; font-weight: 700;">
1. Core Components of Experimental Design
</h3>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
Understanding the <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">essential elements</strong> of scientific experiments allows you to analyze any experimental setup and answer design questions systematically.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Hypothesis and Research Question
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
Every experiment begins with a <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">hypothesis</strong>a testable prediction: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Research question</strong>: A broad inquiry like "How does temperature affect plant growth?" or "What factors influence reaction rate?" The research question identifies what the experiment investigates. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Hypothesis</strong>: A specific, testable prediction answering the research question. "If temperature increases, then plant growth rate will increase" or "Higher enzyme concentration will increase reaction rate." Hypotheses predict a relationship between variables. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Characteristics of good hypotheses</strong>: (a) Testablecan be supported or refuted by experiment. (b) Specificidentifies variables and predicted relationship. (c) Falsifiablecould potentially be proven wrong. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT questions</strong> might ask "What hypothesis was the experiment testing?" Look for statements predicting how changing one variable affects another. Or "Does the data support the hypothesis?" Compare predicted outcomes to actual results. If the hypothesis predicted X would increase Y, and data shows Y decreased as X increased, the hypothesis is not supported. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Key distinction</strong>: The hypothesis guides experimental design. All procedures should logically test the hypothesis. If an experiment manipulates temperature but the hypothesis concerns pH, the design doesn't match the hypothesis.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Independent, Dependent, and Controlled Variables
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
Variables are the <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">core of experimental design</strong>: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Independent variable (IV)</strong>: The factor the experimenter deliberately changes or manipulates. This is the cause in a cause-effect relationship. If testing how fertilizer amount affects plant height, fertilizer amount is the IV. Typically plotted on the x-axis of graphs. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Dependent variable (DV)</strong>: The factor that responds to changes in the independent variable. This is the effect being measured. In the fertilizer example, plant height is the DVit depends on fertilizer amount. Typically plotted on the y-axis. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Controlled variables (constants)</strong>: All other factors that could affect the dependent variable but are kept the same across all experimental groups. For the plant experiment: same type of plant, same light, same water amount, same soil type, same temperature, same pot size, etc. Controlling these prevents them from confounding results. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Why control matters</strong>: If you change fertilizer amount AND light intensity at the same time, you can't determine which factor caused observed changes in growth. Valid experiments change only the independent variable while holding everything else constant. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT questions</strong>: "What was the independent variable?" Identify what the experimenters intentionally varied. "What was the dependent variable?" Identify what was measured as the outcome. "What should be kept constant?" List factors that could influence results but weren't being tested. Recognizing these three variable types is fundamental to understanding any experiment.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Control Groups and Experimental Groups
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Control groups provide a baseline</strong> for comparison: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Experimental group(s)</strong>: Receive the treatment or manipulation. If testing a new drug, experimental groups receive different doses of the drug. If testing temperature effects, experimental groups are exposed to different temperatures. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Control group</strong>: Does NOT receive the experimental treatment but is otherwise treated identically. For a drug test, the control receives a placebo (inert substance). For temperature test, control is kept at room temperature or a standard baseline temperature. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Purpose of controls</strong>: (a) Establish what happens without the experimental treatment. (b) Demonstrate that observed effects are due to the independent variable, not other factors. (c) Provide a reference point for comparison. If the experimental group shows 50% improvement but the control shows 45% improvement, the treatment only added 5%much of the change was due to other factors. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Types of controls</strong>: (a) Negative controlexpected to produce no effect (placebo, no treatment). (b) Positive controlexpected to produce a known effect (standard treatment that works). (c) Baseline/standard condition control (like room temperature when testing heat effects). (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT scenarios</strong>: Questions ask "What was the purpose of the control group?" Answer: to provide a comparison baseline showing what happens without treatment. Or "How could this experiment be improved?" If there's no control group, answer: add a control group receiving no treatment to compare against experimental groups. Understanding that experiments compare treated groups against untreated controls is essential.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Sample Size, Replication, and Statistical Validity
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Good experiments use multiple samples and replicates</strong>: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Sample size</strong>: The number of subjects or units in each experimental group. Testing one plant per group is unreliable; testing 20 plants per group is better. Larger samples reduce the impact of individual variation and outliers. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Replication</strong>: Repeating the entire experiment multiple times (multiple trials). If you run the experiment once and get results, repeating it three more times and getting similar results confirms findings. Replication increases confidence that results weren't due to chance. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Why size and replication matter</strong>: Biological and physical systems have natural variability. One plant might grow unusually fast or slow due to factors unrelated to treatment. Averaging results from many plants gives a more accurate picture. Multiple trials demonstrate consistency. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Statistical significance</strong>: Larger samples and multiple replicates allow statistical tests to determine if differences between groups are real or due to random chance. The ACT doesn't require calculating statistics, but understands that "statistically significant" means results are unlikely due to chance. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT questions</strong>: "How could the experiment be improved?" Common answer: increase sample size (test more subjects per group) or increase number of trials (repeat experiment multiple times). "Why did researchers use 50 plants in each group?" Answer: larger sample size increases reliability and reduces effects of individual variation. Recognizing that more data (more subjects, more trials) makes experiments more reliable is a key concept.
</p>

<div style="background-color: #f8f9fa; border-left: 4px solid #3b82f6; padding: 1rem; margin: 1.5rem 0;">
<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0;">
<strong>Example: Identifying Experimental Components</strong><br><br>
<strong>Experiment:</strong> Researchers tested whether different amounts of sunlight affect tomato plant growth. They placed 10 plants in full sun, 10 in partial shade, and 10 in full shade. All plants were the same variety, planted in identical soil, watered with 200 mL daily, and kept at 22°C. After 4 weeks, average plant height in each group was measured.<br><br>
<strong>Research Question:</strong> How does sunlight amount affect tomato plant growth?<br><br>
<strong>Hypothesis:</strong> Tomato plants receiving more sunlight will grow taller (implied)<br><br>
<strong>Independent Variable:</strong> Amount of sunlight (full sun, partial shade, full shade)<br><br>
<strong>Dependent Variable:</strong> Plant height after 4 weeks<br><br>
<strong>Controlled Variables:</strong> Plant variety, soil type, water amount (200 mL/day), temperature (22°C), time period (4 weeks), number of plants per group (10)<br><br>
<strong>Experimental Groups:</strong> Full sun group, partial shade group, full shade group<br><br>
<strong>Control Group:</strong> The "full sun" group could serve as a positive control (optimal normal growth), or "partial shade" could be considered the baseline. Alternatively, you could argue there's no true control since all groups receive a treatment levela pure control might be indoor plants with artificial standard lighting.<br><br>
<strong>Sample Size:</strong> 10 plants per group (good, but could be improved by increasing to 20-30)<br><br>
<strong>Replication:</strong> Not explicitly statedthey could improve by repeating the entire experiment multiple times<br><br>
<strong>Potential ACT Questions:</strong><br>
1. What was the independent variable? ’ Sunlight amount<br>
2. What should be kept constant? ’ Plant variety, water, temperature, soil, time<br>
3. How could this be improved? ’ Increase sample size or repeat experiment (add trials)
</p>
</div>

<h3 style="margin-top: 5rem; margin-bottom: 0.75rem; font-weight: 700;">
2. Common Experimental Methods and Procedures
</h3>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
Different research questions require <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">different experimental approaches</strong>, each with specific strengths and considerations.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Controlled Experiments: Manipulating One Variable
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Controlled experiments</strong> are the gold standard for establishing causation: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Structure</strong>: The experimenter actively manipulates the independent variable (assigns subjects to different treatment levels) while controlling all other variables. This allows causation to be determinedchanges in the dependent variable are caused by changes in the independent variable. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Example</strong>: To test if temperature affects enzyme activity, prepare enzyme solutions at 10°C, 20°C, 30°C, and 40°C. All solutions have the same enzyme concentration, same substrate concentration, same pH, same volume. Measure reaction rate at each temperature. Because only temperature varies, any differences in reaction rate must be caused by temperature. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Random assignment</strong>: In experiments with living subjects, randomly assigning subjects to groups ensures groups are equivalent at the start, preventing bias. If healthier plants are all placed in the fertilized group, you can't determine if results are due to fertilizer or initial health. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Advantages</strong>: Can establish cause-and-effect relationships. High internal validity (results are due to what you think they're due to). Can isolate effects of single variables. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Limitations</strong>: Can be artificial (lab conditions don't match real-world conditions). Can't be used for unethical manipulations (can't expose humans to toxins to study effects). Sometimes not feasible (can't manipulate earthquake magnitude to study effects). (6) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT context</strong>: Most ACT experiments are controlled experiments. Questions test whether you recognize proper control, whether variables are properly isolated, and whether conclusions about causation are justified.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Observational Studies: Measuring Without Manipulation
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Observational studies</strong> observe and measure without experimental manipulation: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Structure</strong>: Researchers measure variables in their natural state without intervening. For example, observing bird populations in different forest types, measuring pollution levels in various lakes, or recording star temperatures and luminosities. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">When used</strong>: (a) Manipulation is impossible (can't manipulate star temperature, can't change Earth's atmosphere composition). (b) Manipulation is unethical (can't expose people to carcinogens). (c) Studying natural phenomena in realistic settings (animal behavior in wild vs. lab). (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Correlation vs. causation</strong>: Observational studies can identify correlations (two variables are associated) but cannot definitively prove causation because many uncontrolled variables might be involved. If you observe that lakes near farms have more algae, you can't be certain fertilizer runoff caused itmaybe those lakes are also shallower (confounding variable). (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Advantages</strong>: Realistic, natural conditions. Can study phenomena that can't be manipulated. Often less expensive and faster than controlled experiments. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Limitations</strong>: Cannot establish causation conclusively. Many confounding variables. Less control means more uncertainty about what caused observed patterns. (6) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT questions</strong>: May ask you to identify studies as observational. "Can the researchers conclude that X caused Y?" If it's an observational study, the answer is noonly correlation can be established. Understanding that observation shows relationships but not definitive causation is important.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Comparative Experiments: Testing Multiple Levels
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
Many experiments <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">compare multiple levels</strong> of the independent variable: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Purpose</strong>: To determine not just IF a variable has an effect, but HOW the effect changes across a range of values. Does the relationship increase linearly? Is there an optimal level? Does the effect plateau? (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Example</strong>: Instead of just testing "fertilizer vs. no fertilizer," test 0 g, 5 g, 10 g, 15 g, and 20 g of fertilizer per plant. This reveals whether more is always better, or if there's an optimal amount beyond which additional fertilizer doesn't help (or harms). (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Dose-response experiments</strong>: Common in medicine and toxicology. Test different doses of a drug or toxin to see how response varies with concentration. Reveals relationships like "low doses help, high doses harm" or "no effect until a threshold dose, then strong effect." (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Advantages</strong>: Provides detailed understanding of the relationship between variables. Can identify optimal conditions, thresholds, or limits. Generates data for graphs showing trends. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT context</strong>: Many ACT experiments test 3-5 levels of the independent variable. Tables and graphs show how the dependent variable changes across these levels. Questions ask you to identify trends (linear increase, plateau, inverse relationship) or predict outcomes at untested levels (interpolation or extrapolation based on the trend). Understanding that multiple test levels reveal the shape of the relationship is key.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Factorial Designs: Testing Multiple Variables Simultaneously
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Factorial experiments</strong> manipulate two or more independent variables at once: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Structure</strong>: Test all combinations of levels of different variables. For example, test three light levels (low, medium, high) combined with two water levels (dry, wet) for six total treatment combinations: low light-dry, low light-wet, medium light-dry, medium light-wet, high light-dry, high light-wet. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Purpose</strong>: (a) Determine individual effects of each variable. (b) Determine if variables interact (the effect of one variable depends on the level of another). For instance, fertilizer might help plants only when they also receive adequate waterfertilizer and water interact. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Interactions</strong>: If high fertilizer increases growth when water is high but decreases growth when water is low, that's an interaction. The effect of fertilizer depends on water level. Interactions are important biological and chemical phenomena. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Advantages</strong>: Efficientcan study multiple variables in one experiment. Reveals interactions that separate experiments would miss. Reflects real-world complexity where multiple factors act together. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Challenges</strong>: More complex design and analysis. Requires many groups (3 levels × 2 levels = 6 groups; 3 levels × 3 levels = 9 groups). Can be confusing to interpret when interactions are present. (6) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT context</strong>: Some passages present factorial experiments with tables showing results for all combinations. Questions might ask "What is the effect of temperature when pH is high?" requiring you to look at the subset of data where pH = high. Or "Do temperature and pH interact?" requiring you to see if temperature's effect differs at different pH levels. Recognizing factorial designs helps you navigate complex data tables.
</p>

<div style="background-color: #f8f9fa; border-left: 4px solid #3b82f6; padding: 1rem; margin: 1.5rem 0;">
<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0;">
<strong>Example: Comparing Experimental Methods</strong><br><br>
<strong>Scenario: Studying the effect of exercise on heart rate</strong><br><br>
<strong>Controlled Experiment:</strong><br>
Randomly assign 60 people to three groups: no exercise (control), moderate exercise (20 min walking), intense exercise (20 min running). Measure heart rate after exercise. All participants same age range, same time of day, same environment.<br>
Result: Can conclude exercise intensity causes different heart rates.<br><br>
<strong>Observational Study:</strong><br>
Survey 200 people about their typical exercise habits and measure their resting heart rate. Compare heart rates of self-reported non-exercisers, moderate exercisers, and intense exercisers.<br>
Result: Can identify correlation (exercisers have different heart rates) but can't prove causationmaybe people with naturally lower heart rates are more inclined to exercise.<br><br>
<strong>Comparative Experiment:</strong><br>
Test 5 exercise durations: 5 min, 10 min, 15 min, 20 min, 25 min of running. Measure heart rate after each duration.<br>
Result: Shows how heart rate changes with exercise durationreveals the dose-response relationship.<br><br>
<strong>Factorial Design:</strong><br>
Test 3 exercise types (walking, jogging, running) × 2 durations (15 min, 30 min) = 6 groups.<br>
Result: Determines effects of both exercise type and duration, plus whether they interact (e.g., maybe duration matters more for walking than running).
</p>
</div>

<h3 style="margin-top: 5rem; margin-bottom: 0.75rem; font-weight: 700;">
3. Identifying Flawed Designs and Confounding Variables
</h3>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
ACT questions frequently ask you to <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">identify problems</strong> in experimental design and <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">suggest improvements</strong>.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Confounding Variables: When Multiple Factors Vary Together
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Confounding occurs</strong> when an uncontrolled variable changes along with the independent variable: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Definition</strong>: A confounding variable is a factor that affects the dependent variable and varies systematically with the independent variable, making it impossible to determine which variable caused observed effects. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Example</strong>: Testing fertilizer effects by placing fertilized plants in a greenhouse and unfertilized plants outdoors. Temperature and light differ between locations. If fertilized plants grow better, is it due to fertilizer, or warmth, or light? These factors are confounded. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Common confounds</strong>: (a) Time-related confoundsif you test treatment A in winter and treatment B in summer, season confounds treatment. (b) Location confoundsdifferent locations may have different light, temperature, soil, predators, etc. (c) Subject characteristicsif you test drug on adults and placebo on children, age confounds treatment. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Impact</strong>: Confounding ruins internal validity. You cannot interpret results because alternative explanations exist. Any conclusion about the independent variable is questionable. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Prevention</strong>: Control variables by keeping them constant across all groups. Randomize assignment so confounds distribute evenly across groups. Use matched pairs (subjects similar in potential confounding variables) or statistical controls. (6) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT questions</strong>: "What is a confounding variable in this experiment?" Identify factors that differ between groups other than the intended independent variable. "How could confounding be eliminated?" Answer: control that variable by making it the same across all groups, or randomize to distribute its effects evenly.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Lack of Control Group
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Experiments without controls</strong> lack a comparison baseline: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Problem</strong>: If you give all participants a drug and measure improvement, you don't know if improvement was due to the drug or other factors like natural recovery, placebo effect, or time. Without a control group receiving no drug (or placebo), you have no reference point. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Example</strong>: Testing a new fertilizer by applying it to 20 plants and measuring growth. Plants grow 15 cm on average. Is this good? You don't know without a control group receiving no fertilizer to compare against. Maybe unfertilized plants would have grown 14 cm (fertilizer barely helped) or maybe 5 cm (fertilizer helped a lot). (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Solution</strong>: Include a control group treated identically except for the experimental treatment. For drug studies, give a placebo. For fertilizer studies, include unfertilized plants. For temperature studies, maintain a room-temperature control. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Types of no-control errors</strong>: (a) All groups receive treatmentno untreated baseline. (b) Comparing to historical data (last year's results) rather than a concurrent controltoo many factors change over time. (c) Assuming a theoretical baseline without empirical control measurement. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT questions</strong>: "What flaw exists in this experimental design?" Answer: no control group. "How could this experiment be improved?" Answer: add a control group receiving no treatment to provide a comparison baseline. Recognizing missing controls is one of the most common ACT experimental design questions.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Inadequate Sample Size or Replication
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Too few subjects or trials</strong> reduce reliability: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Problem</strong>: Small sample sizes are vulnerable to individual variation and outliers. If you test only 2 plants per group, one unusually large plant drastically affects the average. Results might not represent the true effect. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Single trial problem</strong>: Running an experiment once provides limited confidence. Maybe that particular trial had unusual conditions. Repeating the experiment and getting consistent results validates findings. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Statistical issues</strong>: Small samples reduce statistical powerability to detect real effects. Small samples increase the role of chance. Conclusions from small samples are less generalizable. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">How much is enough?</strong>: Depends on variability in the system. High variability requires larger samples. Common guidelines: at least 10-30 subjects per group for biological systems, at least 3-5 trials for physical science experiments. The ACT doesn't specify numbers but asks conceptually whether sample size is adequate. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Solution</strong>: Increase sample size (more subjects per group) and/or increase replication (more trials of the entire experiment). (6) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT questions</strong>: "The experiment used 3 plants per group. What is a limitation of this design?" Answer: small sample size increases effects of individual variation, reducing reliability. "How could reliability be increased?" Answer: increase sample size or number of trials. Understanding that "more data is better" is a fundamental principle.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Measurement Issues and Bias
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">How measurements are made</strong> affects data quality: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Precision and accuracy</strong>: Measurements should be both precise (consistent, repeatable) and accurate (close to true value). Using a poorly calibrated instrument gives inaccurate data. Inconsistent measurement technique gives imprecise data. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Subjective vs. objective measures</strong>: Objective measures (temperature with thermometer, mass with scale) are preferred because they're quantifiable and reproducible. Subjective measures (rating pain on 1-10 scale, judging color as "dark" or "light") are vulnerable to bias and inconsistency. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Observer bias</strong>: If experimenters know which group is which, they might unconsciously measure or record data differently. Blinding (keeping experimenters unaware of group assignments until after measurement) prevents this. Double-blind experiments (neither subjects nor experimenters know group assignments until the end) are ideal for human studies. (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Timing of measurement</strong>: Measuring at inconsistent times introduces variability. If you measure one group in the morning and another in the afternoon, circadian rhythms might affect results. Standardize measurement timing. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT questions</strong>: "What is a potential source of bias in this experiment?" Look for subjective measurements, knowledge of group assignments by experimenters, or inconsistent measurement protocols. "How could measurement be improved?" Answer: use objective instruments, standardize timing, blind observers to group assignments, calibrate instruments. Recognizing that measurement quality affects data validity is important for evaluating experiments.
</p>

<div style="background-color: #f8f9fa; border-left: 4px solid #3b82f6; padding: 1rem; margin: 1.5rem 0;">
<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0;">
<strong>Example: Identifying and Fixing Flawed Designs</strong><br><br>
<strong>Flawed Experiment 1: Confounding</strong><br>
Design: To test if a new teaching method improves test scores, researchers use the new method in School A and the old method in School B. After one semester, School A students score higher.<br>
Problem: Schools confounded with teaching method. School A might have better funding, more experienced teachers, different student populations, etc.<br>
Fix: Use both methods within the same school. Randomly assign half the classes to new method, half to old method. This controls for school-level variables.<br><br>
<strong>Flawed Experiment 2: No Control</strong><br>
Design: Test a weight loss supplement by having 30 people take it for 8 weeks while also exercising 3x/week. Participants lose an average of 5 pounds.<br>
Problem: No control group. Can't determine if weight loss was due to supplement or exercise.<br>
Fix: Add three groups: (1) supplement + exercise, (2) placebo + exercise, (3) placebo + no exercise. Compare results to isolate effects of supplement.<br><br>
<strong>Flawed Experiment 3: Small Sample</strong><br>
Design: Test if fertilizer affects plant growth using 2 plants with fertilizer and 2 without.<br>
Problem: Only 2 plants per group. Individual variation dominates. One unusually large or small plant skews results.<br>
Fix: Increase to 20-30 plants per group. Average results will be more representative.<br><br>
<strong>Flawed Experiment 4: Measurement Bias</strong><br>
Design: Researcher rates "health" of plants on a 1-5 scale, knowing which plants received treatment.<br>
Problem: Subjective rating + observer knows treatment = potential bias. Researcher might unconsciously rate treated plants higher.<br>
Fix: (1) Use objective measure (plant height, biomass). (2) If using ratings, have a blind observer who doesn't know which plants were treated.
</p>
</div>

<h3 style="margin-top: 5rem; margin-bottom: 0.75rem; font-weight: 700;">
4. Strategic Approaches to Experimental Design Questions
</h3>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Systematic strategies</strong> help you efficiently analyze experimental setups and answer ACT design questions.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
The Variable Checklist: Quickly Analyzing Any Experiment
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
Use this <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">mental checklist</strong> when reading experimental passages: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">What is being manipulated?</strong> Identify the independent variablewhat did experimenters change on purpose? This is usually clear in the methods section: "Plants were exposed to four different temperatures..." ’ IV is temperature. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">What is being measured?</strong> Identify the dependent variablewhat outcome are they recording? "Growth rate was measured..." ’ DV is growth rate. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">What is being held constant?</strong> List controlled variables mentioned in the passage: "All plants were the same species, received 100 mL water daily, and were kept at pH 7." (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Is there a control group?</strong> Look for an untreated or baseline group. If one level is "no treatment" or "room temperature" or "placebo," that's likely the control. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Are sample sizes adequate?</strong> Note how many subjects per group and whether multiple trials were conducted. (6) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Are there obvious confounds?</strong> Look for variables that changed along with the IV but weren't controlled. Answering these six questions gives you a complete understanding of the experimental structure and prepares you for most design questions.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Matching Procedures to Hypotheses
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Good experiments directly test their stated hypotheses</strong>: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Hypothesis specifies variables</strong>: "Increasing temperature will increase reaction rate" identifies temperature (IV) and reaction rate (DV). The experiment must manipulate temperature and measure reaction rate. (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Check for alignment</strong>: Does the IV in the experiment match the variable mentioned in the hypothesis? Does the DV match the predicted outcome? If hypothesis concerns temperature but experiment manipulates pressure, there's a mismatch. (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT question type</strong>: "Does this experiment test the stated hypothesis?" Analyze whether manipulated and measured variables match the hypothesis. If they don't, answer "No, because the experiment manipulates X but the hypothesis concerns Y." (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Alternative designs</strong>: If asked "What experiment would better test this hypothesis?", propose a design that directly manipulates the variable in the hypothesis and measures the predicted outcome. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Common mismatch</strong>: Hypothesis predicts a mechanism or process, but experiment only measures end results without testing the mechanism. "Fertilizer increases growth by improving nitrogen uptake" requires measuring nitrogen uptake, not just final growth. This logical connection between hypothesis and experimental design is testable on the ACT.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Improving Experiments: Common Suggestions
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Standard improvements</strong> answer "How could this experiment be improved?" questions: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Add a control group</strong>: If missing, this is the #1 improvement. "Include a group receiving no treatment to provide a comparison baseline." (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Increase sample size</strong>: "Test more subjects per group to reduce effects of individual variation and increase reliability." (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Add replication</strong>: "Repeat the experiment multiple times to confirm results are consistent and not due to chance." (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Control confounding variables</strong>: "Keep [specific variable] constant across all groups to prevent it from confounding results." Or "Randomize assignment to distribute confounding variables evenly." (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Improve measurement</strong>: "Use objective instruments instead of subjective ratings" or "Standardize measurement timing" or "Blind observers to group assignments to prevent bias." (6) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Test more levels</strong>: "Include additional values of the independent variable to better understand the relationship" (e.g., if only testing 0 and 100, add 25, 50, 75). (7) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Extend time</strong>: "Increase duration to observe long-term effects" (if the experiment is very short). These are the most common valid improvements on ACT questions. Recognizing which apply to a given experiment allows you to answer improvement questions confidently.
</p>

<h4 style="margin-top: 2rem; margin-bottom: 0.3rem; font-weight: 400; font-size: 16px;">
Evaluating Whether Conclusions Are Justified
</h4>

<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0 1rem 0;">
<strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Not all conclusions follow logically</strong> from experimental results: (1) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Overgeneralization</strong>: Concluding more than the data support. If you test one plant species, concluding "all plants respond this way" is overgeneralization. Justified conclusion: "This species responds this way." (2) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Claiming causation from correlation</strong>: If the study was observational, causation cannot be claimed. "X and Y are correlated" is justified; "X causes Y" is not (unless it's a controlled experiment). (3) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Ignoring alternative explanations</strong>: If confounding variables exist, alternative explanations are possible. The conclusion should acknowledge this: "Results suggest X may cause Y, but further research controlling for Z is needed." (4) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Extrapolating beyond tested range</strong>: If temperature was tested from 20-40°C, concluding what happens at 0°C or 100°C is extrapolation. It might be reasonable, but it's not directly supported by the data. (5) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">ACT questions</strong>: "Is this conclusion justified based on the experimental design?" Evaluate whether the conclusion matches what was actually tested and controlled. "What is a limitation of this conclusion?" Identify overgeneralizations, confounds, or assumptions not tested. (6) <strong style="color: #2563eb; font-weight: 600; text-decoration: underline;">Good conclusions</strong>: Are specific to tested conditions, acknowledge limitations, distinguish correlation from causation (when appropriate), and avoid claims beyond the data. Training yourself to critique conclusions develops scientific thinking valued by the ACT.
</p>

<div style="background-color: #f8f9fa; border-left: 4px solid #3b82f6; padding: 1rem; margin: 1.5rem 0;">
<p style="font-size: 16px; line-height: 1.7; margin: 0.5rem 0;">
<strong>Example: Strategic Analysis of an Experiment</strong><br><br>
<strong>Passage Summary:</strong> Researchers hypothesized that caffeine improves memory. They gave 20 college students a memory test, then had them drink coffee, then retested memory 30 minutes later. Average scores increased from 72% to 78%.<br><br>
<strong>Variable Checklist:</strong><br>
1. What's manipulated? ’ Caffeine (before/after drinking coffee)<br>
2. What's measured? ’ Memory test scores<br>
3. What's controlled? ’ Same subjects (before/after design), same test timing (30 min after), college students<br>
4. Control group? ’ NOno group that didn't drink coffee<br>
5. Sample size? ’ 20 subjects (moderate, could be improved)<br>
6. Confounds? ’ Practice effect (second test might be higher just from taking test twice), expectation (students might try harder after coffee), unknown<br><br>
<strong>Hypothesis Match:</strong><br>
Hypothesis concerns caffeine ’ Experiment gives caffeine <br>
Hypothesis concerns memory ’ Experiment measures memory <br>
Match is reasonable, but design has flaws<br><br>
<strong>Potential Improvements:</strong><br>
1. Add control group receiving decaf coffee (controls for practice effect, liquid, ritual)<br>
2. Use double-blind design (students don't know if they got regular or decaf; scorer doesn't know which group is which)<br>
3. Increase sample size to 50-100<br>
4. Randomize whether students get caffeine or decaf first (if using crossover design)<br><br>
<strong>Conclusion Evaluation:</strong><br>
Stated conclusion: "Caffeine improves memory"<br>
Justified? ’ NO<br>
Problems: (a) No controlimprovement might be practice effect. (b) Correlation onlystudents expected coffee to help, so they might have concentrated harder. (c) Limited generalizationonly college students tested.<br>
Better conclusion: "Memory test scores increased after coffee consumption, but further research with a control group is needed to determine if caffeine specifically caused the improvement."<br><br>
<strong>ACT Question Examples:</strong><br>
Q: "What is a major flaw in this design?"<br>
A: "No control group to account for practice effects from taking the test twice"<br><br>
Q: "How could this experiment be improved?"<br>
A: "Include a control group receiving decaffeinated coffee" OR "Increase sample size"<br><br>
Q: "Is the conclusion that caffeine improves memory justified?"<br>
A: "No, because alternative explanations (practice effect, expectation) were not controlled"
</p>
</div>

<!-- ========================================
     SECTION 3: HIDDEN SEPARATOR
     ======================================== -->

<h3 style="visibility: hidden; margin: 0; padding: 0; line-height: 0; height: 0;">
Hidden Separator
</h3>

<!-- ========================================
     SECTION 4: KEY TAKEAWAYS
     ======================================== -->

<h3 style="color: #2e7d32; font-size: 1.4rem; font-weight: 700; margin: 3rem 0 1.5rem 0;">
Key Takeaways
</h3>

<ul style="list-style: none; padding: 0; margin: 0;">
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Independent variable (IV) is what experimenters manipulate; dependent variable (DV) is what they measure as the outcome
  </li>
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Controlled variables must be kept constant across all groups to prevent confounding and ensure valid conclusions
  </li>
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Control groups provide untreated baseline for comparison; without controls, you can't determine if treatment caused effects
  </li>
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Larger sample sizes and multiple trials (replication) increase reliability and reduce impact of individual variation
  </li>
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Controlled experiments can establish causation; observational studies show correlation but not definitive causation
  </li>
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Confounding variables change along with IV and provide alternative explanations for results, ruining internal validity
  </li>
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Common design flaws: no control group, confounding variables, inadequate sample size, subjective measurements, observer bias
  </li>
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Standard improvements: add control group, increase sample size, add replication, control confounds, improve measurement objectivity
  </li>
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Experimental procedures must align with hypothesisvariables manipulated and measured should match those stated in hypothesis
  </li>
  <li style="margin-bottom: 0.8rem; color: #2e7d32; font-size: 16px; line-height: 1.6;">
    <span style="color: #4caf50; font-weight: bold; margin-right: 0.5rem;"></span>Conclusions must be justified by dataavoid overgeneralization, claiming causation without controls, or extrapolating beyond tested conditions
  </li>
</ul>
